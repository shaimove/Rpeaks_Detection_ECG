{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wfdb\n",
    "#!pip install torch-summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used image is: Pytorch 1.10 Python Python 3.8 GPU optimized.\n",
    "\n",
    "On AWS kernel: ml.g4dn.xlarge (4 vCPU + 16 GiB + 1 GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing needed libraries\n",
    "from matplotlib import pyplot as plt\n",
    "from wfdb.io import get_record_list\n",
    "from wfdb import rdsamp\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.signal import resample_poly\n",
    "from pickle import dump, load\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import recall_score\n",
    "import tqdm\n",
    "\n",
    "import utils\n",
    "from data_generator import dataset_gen\n",
    "from model_lstm import lstm_model, cnn_lstm_model\n",
    "from model_unet import unet_model, unet_inception_model\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_flag = False\n",
    "if load_flag:\n",
    "    # import files\n",
    "    mitdb_records = get_record_list('mitdb')\n",
    "    mitdb_signals, mitdb_beats, mitdb_beat_types = utils.data_from_records(mitdb_records, channel=0, db='mitdb')\n",
    "    # save file\n",
    "    dump(mitdb_signals, open('./mitdb_signals.pkl', 'wb'))\n",
    "    dump(mitdb_beats, open('./mitdb_beats.pkl', 'wb'))\n",
    "    dump(mitdb_beat_types, open('./mitdb_beat_types.pkl', 'wb'))\n",
    "else:\n",
    "    mitdb_signals = load(open('./mitdb_signals.pkl', 'rb'))\n",
    "    mitdb_beats = load(open('./mitdb_beats.pkl', 'rb'))\n",
    "    mitdb_beat_types = load(open('./mitdb_beat_types.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different R-peak types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract beat symbols from all records\n",
    "all_symbols = []\n",
    "for symbols in mitdb_beat_types:\n",
    "    all_symbols.append(symbols)\n",
    "    \n",
    "all_symbols = [item for sublist in all_symbols for item in sublist]\n",
    "all_symbols = np.asarray(all_symbols)\n",
    "u, c = np.unique(all_symbols, return_counts=True)\n",
    "\n",
    "# Meanings for different heart beat codings\n",
    "label_meanings = {\n",
    "    \"N\": \"Normal beat\",\n",
    "    \"L\": \"Left bundle branch block beat\",\n",
    "    \"R\": \"Right bundle branch block beat\",\n",
    "    \"V\": \"Premature ventricular contraction\",\n",
    "    \"/\": \"Paced beat\",\n",
    "    \"A\": \"Atrial premature beat\",\n",
    "    \"f\": \"Fusion of paced and normal beat\",\n",
    "    \"F\": \"Fusion of ventricular and normal beat\",\n",
    "    \"j\": \"Nodal (junctional) escape beat\",\n",
    "    \"a\": \"Aberrated atrial premature beat\",\n",
    "    \"E\": \"Ventricular escape beat\",\n",
    "    \"J\": \"Nodal (junctional) premature beat\",\n",
    "    \"Q\": \"Unclassifiable beat\",\n",
    "    \"e\": \"Atrial escape beat\",\n",
    "    \"S\": \"Supraventricular premature or ectopic\"\n",
    "}\n",
    "\n",
    "# Print number of instances in each beat type\n",
    "label_counts = [(label, count) for label, count in zip(u.tolist(), c.tolist())]\n",
    "label_counts.sort(key=lambda tup: tup[1], reverse=True)\n",
    "for label in label_counts:\n",
    "    print(label_meanings[label[0]], \"-\"*(40-len(label_meanings[label[0]])), label[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot examples of beat types\n",
    "plt.style.use('ggplot')\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_figheight(5), fig.set_figwidth(18)\n",
    "ax1.plot(mitdb_signals[3][:1000])\n",
    "ax1.set_title('Normal beats')\n",
    "ax2.plot(mitdb_signals[23][:1000])\n",
    "ax2.set_title('Premature venticular contractions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting character encodings to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary that encodes characters as numerical\n",
    "label_codings = {}\n",
    "for i in range(0, len(label_counts)):\n",
    "    if label_counts[i][0] == 'N':\n",
    "        label_codings[label_counts[i][0]] = 1\n",
    "    else:\n",
    "        label_codings[label_counts[i][0]] = -1\n",
    "label_codings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save new numerical encodings as list\n",
    "mitdb_labels = []\n",
    "for beat_types in mitdb_beat_types:\n",
    "    numerical_symbols = [label_codings[sym] for sym in beat_types]\n",
    "    mitdb_labels.append(np.asarray(numerical_symbols))\n",
    "    \n",
    "mitdb_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot signal and beat locations as dashed line\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(mitdb_signals[8][:1000])\n",
    "for peak in mitdb_beats[8][:4]:\n",
    "    plt.axvline(x=peak, color='k', linestyle='--', alpha=0.5)\n",
    "plt.title('Normal beats where R-peak location occurs at the bottom of a valley')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitdb_labels = utils.fix_labels(mitdb_signals, mitdb_beats, mitdb_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "baseline_wander = rdsamp('bw', pn_dir='nstdb')\n",
    "muscle_artifact = rdsamp('ma', pn_dir='nstdb')\n",
    "\n",
    "# Concatenate two channels to make one longer recording\n",
    "ma = np.concatenate((muscle_artifact[0][:,0], muscle_artifact[0][:,1]))\n",
    "bw = np.concatenate((baseline_wander[0][:,0], baseline_wander[0][:,1]))\n",
    "\n",
    "# Resample noise to 250 Hz\n",
    "ma = resample_poly(ma, up=250, down=muscle_artifact[1]['fs'])\n",
    "bw = resample_poly(bw, up=250, down=baseline_wander[1]['fs'])\n",
    "\n",
    "# Plot examples of baseline wandering and muscle artifact noise types\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_figheight(7), fig.set_figwidth(18)\n",
    "ax1.plot(bw[:5000])\n",
    "ax1.set_title('Baseline wander')\n",
    "ax2.plot(ma[:5000])\n",
    "ax2.set_title('Muscle artifact')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precent_train = 0.7\n",
    "\n",
    "# create list of train and validation indexs\n",
    "index_list = list(range(len(mitdb_signals)))\n",
    "random.shuffle(index_list)\n",
    "num_assets_train = int(precent_train * len(index_list))\n",
    "index_train = index_list[:num_assets_train]\n",
    "index_validation = index_list[num_assets_train:]\n",
    "\n",
    "print('Number of examples in training is %d and in validation is %d' % (len(index_train),len(index_validation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Data\n",
    "n_batch = 256\n",
    "win_size = 1000\n",
    "\n",
    "train_dataset = dataset_gen(signals=[mitdb_signals[i] for i in index_train], \n",
    "                            peaks=[mitdb_beats[i] for i in index_train], \n",
    "                            labels=[mitdb_labels[i] for i in index_train], \n",
    "                            ma=ma,\n",
    "                            bw=bw,\n",
    "                            win_size=win_size)\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, \n",
    "                               batch_size=n_batch, \n",
    "                               shuffle=True)\n",
    "\n",
    "# Define Validation Data\n",
    "validation_dataset = dataset_gen(signals=[mitdb_signals[i] for i in index_validation], \n",
    "                            peaks=[mitdb_beats[i] for i in index_validation], \n",
    "                            labels=[mitdb_labels[i] for i in index_validation], \n",
    "                            ma=ma,\n",
    "                            bw=bw,\n",
    "                            win_size=win_size)\n",
    "\n",
    "validation_loader = data.DataLoader(validation_dataset, \n",
    "                                    batch_size=n_batch, \n",
    "                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model, loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "output_dim = 1\n",
    "input_dim = 1\n",
    "hidden_size = 64\n",
    "\n",
    "# training hyper parameters\n",
    "num_epochs = 100\n",
    "l1_lambda = 0.001\n",
    "fp_lambda = 1\n",
    "positive_class_weight  = 20\n",
    "negative_class_weight = 0.1\n",
    "\n",
    "# define loss, optimizer and model\n",
    "criterion = nn.BCELoss(reduction='none')\n",
    "#model = lstm_model(input_dim, hidden_size, output_dim).to(device)\n",
    "#model = cnn_lstm_model(conv_input_dim=1, conv_kernel=[3,3,3,3,3], conv_feature = [16,32,32,16,1], hidden_size_lstm = 64).to(device)\n",
    "#model = unet_model(conv_input_dim=1, conv_kernel = [3,3,3], conv_feature = [16,32,64], output_dim = 1).to(device)\n",
    "model = unet_inception_model(conv_input_dim = 1, conv_kernel_res = [15,17,19,21], conv_feature = [32,64,128,256], stride_size = [2,2,2,5], output_dim = 1).to(device)\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.001\n",
    "weight_decay_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay_rate)\n",
    "\n",
    "# empty vectors for loss\n",
    "train_loss_vec = np.zeros(num_epochs)\n",
    "validation_loss_vec = np.zeros(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary(model, (1000,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    ##################\n",
    "    ### TRAIN LOOP ###\n",
    "    ##################\n",
    "    # set the model to train mode, initiate training loss\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for i,batch in enumerate(train_loader):\n",
    "        # get batch images and labels\n",
    "        X, y = batch\n",
    "        \n",
    "        # zero grad, predict\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        \n",
    "        # weighted BCE loss\n",
    "        loss_class = criterion(y_pred,y)\n",
    "        weight = negative_class_weight + y * (positive_class_weight - negative_class_weight)\n",
    "        loss_class = loss_class * weight\n",
    "        loss_class = loss_class.mean()\n",
    "        \n",
    "        # add L1 regularization \n",
    "        l1_norm = 0\n",
    "        for param in model.parameters():\n",
    "            l1_norm += torch.norm(param)\n",
    "        \n",
    "        # add false positive loss\n",
    "        loss_fp = y_pred.mean()\n",
    "        \n",
    "        # sum losses\n",
    "        loss = loss_class + l1_lambda * l1_norm + fp_lambda * loss_fp\n",
    "        \n",
    "        # backward and step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # accumulate the training loss, and print it\n",
    "        train_loss += loss.item()\n",
    "        #if i%1 == 0: print('Epoch %d, Train Batch %d/%d, loss: %.4f' % (epoch,i,len(train_loader),loss))\n",
    "\n",
    "            \n",
    "    #######################\n",
    "    ### VALIDATION LOOP ###\n",
    "    #######################\n",
    "    # set the model to eval mode and initiate validation loss\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    \n",
    "    # turn off gradients for validation\n",
    "    with torch.no_grad():\n",
    "        for i,batch in enumerate(validation_loader):\n",
    "            # get batch images and labels\n",
    "            X,y = batch\n",
    "            \n",
    "            # forward pass\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # weighted BCE loss\n",
    "            loss_class = criterion(y_pred,y)\n",
    "            weight = negative_class_weight + y * (positive_class_weight - negative_class_weight)\n",
    "            loss_class = loss_class * weight\n",
    "            loss_class = loss_class.mean()\n",
    "        \n",
    "            # add L1 regularization \n",
    "            l1_norm = 0\n",
    "            for param in model.parameters():\n",
    "                l1_norm += torch.norm(param)\n",
    "            \n",
    "            # add false positive loss\n",
    "            loss_fp = y_pred.mean()\n",
    "            \n",
    "            # sum losses\n",
    "            loss = loss_class + l1_lambda * l1_norm + fp_lambda * loss_fp\n",
    "        \n",
    "            # accumulate the valid_loss and print it\n",
    "            valid_loss += loss.item()\n",
    "            #if i % 1 == 0: print('Epoch %d, Validation Batch %d/%d, loss: %.4f' % (epoch,i,len(validation_loader),loss))\n",
    "                \n",
    "                \n",
    "    #########################\n",
    "    ## PRINT EPOCH RESULTS ##\n",
    "    #########################\n",
    "    train_loss /= len(train_loader)\n",
    "    valid_loss /= len(validation_loader)\n",
    "    # update training and validation loss\n",
    "    train_loss_vec[epoch] = train_loss\n",
    "    validation_loss_vec[epoch] = valid_loss\n",
    "    # print results\n",
    "    print('Epoch: %s/%s: Training loss: %.3f. Validation Loss: %.3f.'\n",
    "          % (epoch+1,num_epochs,train_loss,valid_loss))\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print('Weighted BCE loss is %.3f, FP loss is %.3f, L1 reg loss is %.3f' % (loss_class, fp_lambda * loss_fp, l1_lambda * l1_norm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4 training examples with labels\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.set_figheight(10), fig.set_figwidth(18)\n",
    "fig.suptitle('Some examples of training data with labels', size=14)\n",
    "\n",
    "# convet to cuda\n",
    "X = X.cpu().numpy()\n",
    "y = y.cpu().numpy()\n",
    "y_pred = y_pred.cpu().numpy()\n",
    "\n",
    "\n",
    "axs[0, 0].plot(X[0,:,:])\n",
    "axs[0, 0].plot(y[0,:,:]+1)\n",
    "axs[0, 0].plot(y_pred[0,:,:]+2)\n",
    "\n",
    "axs[0, 1].plot(X[1,:,:])\n",
    "axs[0, 1].plot(y[1,:,:]+1)\n",
    "axs[0, 1].plot(y_pred[1,:,:]+2)\n",
    "\n",
    "axs[1, 0].plot(X[2,:,:])\n",
    "axs[1, 0].plot(y[2,:,:]+1)\n",
    "axs[1, 0].plot(y_pred[2,:,:]+2)\n",
    "\n",
    "axs[1, 1].plot(X[3,:,:])\n",
    "axs[1, 1].plot(y[3,:,:]+1)\n",
    "axs[1, 1].plot(y_pred[3,:,:]+2)\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "plt.plot(range(num_epochs),train_loss_vec,label='Training Loss')\n",
    "plt.plot(range(num_epochs),validation_loss_vec,label='Validation Loss')\n",
    "plt.grid(); plt.xlabel('Number of epochs'); plt.ylabel('Loss')\n",
    "plt.title('Loss for LSTM model')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model Recall and Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "valid_loss = 0\n",
    "y = []\n",
    "y_pred = []\n",
    "\n",
    "# turn off gradients for evalute\n",
    "with torch.no_grad():\n",
    "    for i,batch in enumerate(validation_loader):\n",
    "        # get batch images and labels\n",
    "        X,y_batch = batch\n",
    "        y_pred_batch = model(X)\n",
    "        \n",
    "        # save to vectore\n",
    "        y.append(y_batch.cpu().flatten().tolist())\n",
    "        y_pred.append(torch.round(y_pred_batch.cpu().flatten()).tolist())\n",
    "        \n",
    "\n",
    "# flat arrays\n",
    "y = np.array([item for sublist in y for item in sublist]).astype(int)\n",
    "y_pred = np.array([item for sublist in y_pred for item in sublist]).astype(int)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate recall and specificty\n",
    "recall = recall_score(y, y_pred) * 100\n",
    "specificty = recall_score(y, y_pred, pos_label = 0) * 100\n",
    "print('For gqrs algorithm, Recall is %.3f and Specificity is %.3f' % (recall, specificty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './unet_inception_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
